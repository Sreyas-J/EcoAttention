{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ff1660e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention output:\n",
      " [[2.90747402 3.90747402 4.90747402]\n",
      " [4.         5.         6.        ]\n",
      " [5.09252598 6.09252598 7.09252598]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flash_attention_tiled(Q, K, V, B_r, B_c,lam):\n",
    "    \"\"\"\n",
    "    Implements the FlashAttention algorithm with tiling.\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V: numpy arrays of shape (N, d)\n",
    "        B_r: row block size\n",
    "        B_c: column block size\n",
    "    \n",
    "    Returns:\n",
    "        O: output attention result of shape (N, d)\n",
    "    \"\"\"\n",
    "    N, d = Q.shape\n",
    "    # Initialize accumulators in HBM\n",
    "    O = np.zeros((N, d), dtype=Q.dtype)\n",
    "    m = np.full(N, -np.inf, dtype=Q.dtype)\n",
    "    l = np.zeros(N, dtype=Q.dtype)\n",
    "    \n",
    "    # Number of blocks\n",
    "    T_r = int(np.ceil(N / B_r))\n",
    "    T_c = int(np.ceil(N / B_c))\n",
    "    \n",
    "    # Loop over K, V blocks\n",
    "    for j in range(T_c):\n",
    "        k_start = j * B_c\n",
    "        k_end = min((j+1) * B_c, N)\n",
    "        K_j = K[k_start:k_end]  # shape (B_c, d)\n",
    "        V_j = V[k_start:k_end]  # shape (B_c, d)\n",
    "        \n",
    "        # Loop over Q blocks\n",
    "        for i in range(T_r):\n",
    "            q_start = i * B_r\n",
    "            q_end = min((i+1) * B_r, N)\n",
    "            Q_i = Q[q_start:q_end]       # shape (B_r, d)\n",
    "            O_i = O[q_start:q_end]       # shape (B_r, d)\n",
    "            m_i = m[q_start:q_end]       # shape (B_r,)\n",
    "            l_i = l[q_start:q_end]       # shape (B_r,)\n",
    "            \n",
    "            # 1. Compute raw scores S_ij = Q_i K_j^T (shape B_r x B_c)\n",
    "            # print(\"Q_i: \",Q_i,\"\\nK_j:\\n\",K_j.T)\n",
    "            S_ij = Q_i @ K_j.T\n",
    "            # S_ij = np.sum(np.abs(Q_i - K_j), axis=1, keepdims=True) \n",
    "            # print(\"\\nS_ij\\n\",S_ij)\n",
    "            \n",
    "            # 2. Block-wise row max for numerical stability\n",
    "            m_tilde = np.max(S_ij, axis=1)\n",
    "            \n",
    "            # 3. Exponentiate shifted scores\n",
    "            P_tilde = np.exp(S_ij - m_tilde[0])\n",
    "            \n",
    "            # 4. Block-wise row sum of exponentials\n",
    "            l_tilde = np.sum(P_tilde, axis=1)\n",
    "            \n",
    "            # 5. Update running max and sum\n",
    "            m_new = np.maximum(m_i, m_tilde)\n",
    "            l_new = np.exp(m_i - m_new) * l_i + np.exp(m_tilde - m_new) * l_tilde\n",
    "            \n",
    "            # 6. Accumulate partial outputs\n",
    "            #    (diag(l_new)^{-1}) [ diag(l_i) e^{m_i-m_new} O_i + e^{m_tilde-m_new} P_tilde V_j ]\n",
    "            term1 = (np.exp(m_i - m_new) * l_i)[:, None] * O_i\n",
    "            term2 = np.exp(m_tilde - m_new)[:, None] * (P_tilde @ V_j)\n",
    "            O_update = (term1 + term2) / l_new[:, None]\n",
    "            \n",
    "            # Write-back\n",
    "            O[q_start:q_end] = O_update\n",
    "            m[q_start:q_end] = m_new\n",
    "            l[q_start:q_end] = l_new\n",
    "    \n",
    "    return O\n",
    "\n",
    "# Example usage with 3x3 matrices\n",
    "Q = np.array([[1, 0, 0],\n",
    "              [0, 1, 0],\n",
    "              [0, 0, 1]], dtype=float)\n",
    "\n",
    "K = Q.copy()\n",
    "V = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]], dtype=float)\n",
    "\n",
    "# Choose block sizes\n",
    "B_r, B_c = 1, 1\n",
    "\n",
    "O_fa = flash_attention_tiled(Q, K, V, B_r, B_c,1)\n",
    "\n",
    "# Display results\n",
    "print(\"FlashAttention output:\\n\", O_fa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2de01430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    # scores = (Q @ K.T) / np.sqrt(d_k)            # [seq_q, seq_k]\n",
    "    scores = (Q @ K.T)\n",
    "    weights = np.exp(scores - scores.max(axis=-1, keepdims=True))\n",
    "    weights /= weights.sum(axis=-1, keepdims=True)  # softmax\n",
    "    return weights @ V   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "206f2cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive attention output:\n",
      " [[2.90747402 3.90747402 4.90747402]\n",
      " [4.         5.         6.        ]\n",
      " [5.09252598 6.09252598 7.09252598]]\n"
     ]
    }
   ],
   "source": [
    "O_naive = attention(Q, K, V) \n",
    "print(\"\\nNaive attention output:\\n\", O_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2b7a4",
   "metadata": {},
   "source": [
    "FLASHATTENTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71a5b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention 2 output:\n",
      " (array([[2.90747402, 3.90747402, 4.90747402],\n",
      "       [4.        , 5.        , 6.        ],\n",
      "       [5.09252598, 6.09252598, 7.09252598]]), array([1.55144471, 1.55144471, 1.55144471]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def flash_attention2(Q: np.ndarray, K: np.ndarray, V: np.ndarray,\n",
    "                     block_rows: int, block_cols: int):\n",
    "    \"\"\"\n",
    "    FlashAttention-2 forward pass in NumPy.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries, shape (N, D)\n",
    "        K: Keys,    shape (N, D)\n",
    "        V: Values,  shape (N, D_v)\n",
    "        block_rows: Block size for Q (B_r)\n",
    "        block_cols: Block size for K, V (B_c)\n",
    "    \n",
    "    Returns:\n",
    "        O: Output, shape (N, D_v)\n",
    "        L: Log-sum-exp per query, shape (N,)\n",
    "    \"\"\"\n",
    "    N, D = Q.shape\n",
    "    _, D_v = V.shape\n",
    "    \n",
    "    # Number of tiles\n",
    "    T_r = math.ceil(N / block_rows)\n",
    "    T_c = math.ceil(N / block_cols)\n",
    "    \n",
    "    O = np.zeros((N, D_v), dtype=Q.dtype)\n",
    "    L = np.zeros(N, dtype=Q.dtype)\n",
    "    \n",
    "    for i in range(T_r):\n",
    "        start_r = i * block_rows\n",
    "        end_r = min((i + 1) * block_rows, N)\n",
    "        Qi = Q[start_r:end_r]                     # (B_r_i, D)\n",
    "        B_r_i = Qi.shape[0]\n",
    "        \n",
    "        # Initialize online-softmax accumulators\n",
    "        m = np.full(B_r_i, -np.inf, dtype=Q.dtype)  # running max\n",
    "        l = np.zeros(B_r_i, dtype=Q.dtype)          # running sum exp\n",
    "        O_tilde = np.zeros((B_r_i, D_v), dtype=Q.dtype)\n",
    "        \n",
    "        for j in range(T_c):\n",
    "            start_c = j * block_cols\n",
    "            end_c = min((j + 1) * block_cols, N)\n",
    "            Kj = K[start_c:end_c]                 # (B_c_j, D)\n",
    "            Vj = V[start_c:end_c]                 # (B_c_j, D_v)\n",
    "            \n",
    "            # 1) Raw attention scores\n",
    "            S = Qi @ Kj.T                          # (B_r_i, B_c_j)\n",
    "            \n",
    "            # 2) Update running max\n",
    "            row_max = np.max(S, axis=1)            # (B_r_i,)\n",
    "            new_m = np.maximum(m, row_max)\n",
    "            \n",
    "            # 3) Compute shifted exp\n",
    "            P = np.exp(S - new_m[:, None])         # (B_r_i, B_c_j)\n",
    "            \n",
    "            # 4) Update running sum of exp\n",
    "            l = np.exp(m - new_m) * l + np.sum(P, axis=1)\n",
    "            \n",
    "            # 5) Accumulate unnormalized output\n",
    "            O_tilde = (np.exp(m - new_m)[:, None] * O_tilde) + (P @ Vj)\n",
    "            \n",
    "            # Commit new max\n",
    "            m = new_m\n",
    "        \n",
    "        # 6) Final normalization for this block\n",
    "        O[start_r:end_r] = O_tilde / l[:, None]\n",
    "        L[start_r:end_r] = m + np.log(l)\n",
    "    \n",
    "    return O, L\n",
    "\n",
    "\n",
    "# Example usage with 3x3 matrices\n",
    "Q = np.array([[1, 0, 0],\n",
    "              [0, 1, 0],\n",
    "              [0, 0, 1]], dtype=float)\n",
    "\n",
    "K = Q.copy()\n",
    "V = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]], dtype=float)\n",
    "\n",
    "# Choose block sizes\n",
    "B_r, B_c = 1, 1\n",
    "\n",
    "O_fa = flash_attention2(Q, K, V, B_r, B_c)\n",
    "\n",
    "# Display results\n",
    "print(\"FlashAttention 2 output:\\n\", O_fa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransformerAcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
